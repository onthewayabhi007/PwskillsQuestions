{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: Missing values in a dataset refer to the absence of data for one or more variables in certain observations. There are several reasons for missing values, such as data entry errors, equipment malfunctions, or participants choosing not to provide certain information. It is essential to handle missing values because they can lead to biased or inefficient analyses and may affect the performance of machine learning algorithms.\n",
    "\n",
    "# Some algorithms that are not affected by missing values include:\n",
    "# 1. Decision Trees: Decision trees can handle missing values by treating missingness as just another category during the tree construction process.\n",
    "# 2. Random Forests: Random Forests can handle missing values by using surrogate splits, which are alternative splits that mimic the behavior of the original split for cases with missing values.\n",
    "# 3. Gradient Boosting Machines(GBMs): GBMs can handle missing values by treating missingness as a separate category and learning how to best use it during the boosting process.\n",
    "\n",
    "# Q2: Techniques to handle missing data:\n",
    "# 1. Deletion: Delete rows or columns with missing values.\n",
    "# ```python\n",
    "# # Deleting rows with missing values\n",
    "# df.dropna(axis=0, inplace=True)\n",
    "\n",
    "# # Deleting columns with missing values\n",
    "# df.dropna(axis=1, inplace=True)\n",
    "# ```\n",
    "# 2. Mean/Mode/Median Imputation: Replace missing values with the mean, mode, or median of the non-missing values in the same column.\n",
    "# ```python\n",
    "# # Mean imputation\n",
    "# df['column'].fillna(df['column'].mean(), inplace=True)\n",
    "\n",
    "# # Mode imputation\n",
    "# df['column'].fillna(df['column'].mode()[0], inplace=True)\n",
    "\n",
    "# # Median imputation\n",
    "# df['column'].fillna(df['column'].median(), inplace=True)\n",
    "# ```\n",
    "# 3. Forward/Backward Fill: Fill missing values with the previous(forward fill) or next(backward fill) valid value in the column.\n",
    "# ```python\n",
    "# # Forward fill\n",
    "# df['column'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# # Backward fill\n",
    "# df['column'].fillna(method='bfill', inplace=True)\n",
    "# ```\n",
    "# 4. Interpolation: Fill missing values using interpolation methods like linear interpolation, polynomial interpolation, etc.\n",
    "# ```python\n",
    "# # Linear interpolation\n",
    "# df['column'].interpolate(method='linear', inplace=True)\n",
    "\n",
    "# # Polynomial interpolation\n",
    "# df['column'].interpolate(method='polynomial', order=2, inplace=True)\n",
    "# ```\n",
    "# 5. Model-based Imputation: Predict missing values using a regression or classification model trained on the non-missing values.\n",
    "# ```python\n",
    "\n",
    "# # Create a mask of missing values\n",
    "# mask = df['column'].isnull()\n",
    "\n",
    "# # Split data into two parts: rows with non-missing values and rows with missing values\n",
    "# train = df[~mask]\n",
    "# test = df[mask]\n",
    "\n",
    "# # Train a linear regression model\n",
    "# model = LinearRegression()\n",
    "# model.fit(train[['feature1', 'feature2']], train['column'])\n",
    "\n",
    "# # Predict missing values\n",
    "# df.loc[mask, 'column'] = model.predict(test[['feature1', 'feature2']])\n",
    "# ```\n",
    "\n",
    "# Q3: Imbalanced data refers to a situation where the distribution of classes in a classification problem is uneven, with one class having significantly more instances than the others. If imbalanced data is not handled properly, it can lead to biased models that have poor performance on the minority class . The model may favor the majority class and have lower accuracy, recall, and F1-score for the minority class .\n",
    "\n",
    "# Q4: Upsampling and downsampling are techniques used to address imbalanced data.\n",
    "# - Upsampling involves increasing the number of instances in the minority class to match the number of instances in the majority class . This can be done by duplicating existing instances or generating synthetic samples.\n",
    "# - Downsampling involves decreasing the number of instances in the majority class to match the number of instances in the minority class . This can be done by randomly selecting a subset of instances from the majority class .\n",
    "\n",
    "# For example, let's consider a binary classification problem with classes A and B. Class A has 100 instances, while class B has only 20 instances. To upsample, we can duplicate instances from class B to have 100 instances. To downsample, we can randomly select 20 instances from class A to match the number of instances in class B.\n",
    "\n",
    "# Q5: Data augmentation is a technique used to artificially increase the size of a dataset by creating slightly modified copies of the existing data. It helps in improving the generalization and performance of machine learning models. SMOTE(Synthetic Minority Over-sampling Technique) is a popular data augmentation method for addressing imbalanced datasets.\n",
    "\n",
    "# SMOTE works by generating synthetic samples for the minority class . It selects a sample from the minority class, finds its k nearest neighbors, and creates new samples along the line segments joining the sample and its neighbors. This helps to create a more balanced dataset.\n",
    "\n",
    "# Q6: Outliers in a dataset are observations that significantly deviate from the other data points. They can be caused by various factors like data entry errors, measurement noise, or genuine rare events. It is essential to handle outliers because they can distort statistical analyses and modeling results. Outliers can disproportionately influence the estimated parameters and affect the performance and generalizability of machine learning models.\n",
    "\n",
    "# Q7: Techniques to handle missing data in customer analysis:\n",
    "# 1. Complete Case Analysis: Exclude observations with missing values from the analysis. This is suitable when the missing data is minimal, and removing the incomplete cases does not introduce significant bias.\n",
    "# 2. Mean/Mode/Median Imputation: Replace missing values with the mean, mode, or median of the non-missing values in the same variable. This is applicable when the missingness is random and not associated with the customer characteristics.\n",
    "# 3. Model-based Imputation: Use regression or classification models to predict missing values based on other variables' values. This is suitable when there is a pattern in the missing data and the missingness is related to other customer characteristics.\n",
    "\n",
    "# Q8: Strategies to determine if missing data is missing at random or has a pattern:\n",
    "# 1. Missing Data Visualization: Plot missingness patterns to visualize if there are any systematic patterns in missing values across variables or observations.\n",
    "# 2. Missing Data Mechanism Tests: Conduct statistical tests like Little's MCAR(Missing Completely at Random) test or MNAR(Missing Not at Random) tests to determine if the missing data can be considered random or not .\n",
    "# 3. Correlation Analysis: Examine the correlations between missing values and other variables to identify potential patterns or relationships.\n",
    "\n",
    "# Q9: Strategies to evaluate performance on an imbalanced dataset:\n",
    "# 1. Confusion Matrix and Class Metrics: Evaluate the confusion matrix, including metrics such as accuracy, precision, recall(sensitivity), specificity, and F1-score, to get a comprehensive understanding of model performance for both the majority and minority classes.\n",
    "# 2. ROC Curve and AUC: Plot the Receiver Operating Characteristic(ROC) curve and calculate the Area Under the Curve(AUC) to assess the model's ability to distinguish between the classes.\n",
    "# 3. Precision-Recall Curve: Plot the Precision-Recall curve and calculate metrics like Average Precision Score(AP) to evaluate the model's performance in the context of imbalanced data.\n",
    "# 4. Resampling Techniques: Use resampling techniques like oversampling the minority class , undersampling the majority class, or a combination of both, to balance the dataset and evaluate the model's performance on the balanced data.\n",
    "\n",
    "# Q10: Methods to balance an unbalanced dataset and down-sample the majority class when estimating customer satisfaction:\n",
    "# 1. Random Under-Sampling: Randomly select a subset of instances from the majority class to reduce its\n",
    "\n",
    "# representation in the dataset.\n",
    "# 2. Cluster-Based Under-Sampling: Use clustering algorithms to identify clusters within the majority class and keep only a representative subset of instances from each cluster.\n",
    "# 3. Tomek Links: Identify pairs of instances from different classes that are closest to each other and remove the majority class instance. This helps in reducing overlapping between classes.\n",
    "# 4. NearMiss Algorithm: Select instances from the majority class based on their distance to the minority class instances, such as selecting the instances with the smallest average distance.\n",
    "\n",
    "# Q11: Methods to balance an unbalanced dataset and up-sample the minority class when estimating the occurrence of a rare event:\n",
    "# 1. Random Over-Sampling: Randomly duplicate instances from the minority class to increase its representation in the dataset.\n",
    "# 2. SMOTE(Synthetic Minority Over-sampling Technique): Generate synthetic samples for the minority class by creating new instances along the line segments joining the minority class samples.\n",
    "# 3. ADASYN(Adaptive Synthetic Sampling): Similar to SMOTE, but it introduces more synthetic samples for the minority class instances that are harder to learn.\n",
    "# 4. SMOTE-ENN: Apply SMOTE to up-sample the minority class and then use the Edited Nearest Neighbors(ENN) algorithm to remove noisy samples from both classes.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
